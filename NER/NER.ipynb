{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import pickle\n",
    "import time\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the NER model to our dataset (list of abstracts)\n",
    "# Load data\n",
    "data = pd.read_csv(r'..\\get data\\alldata.csv')\n",
    "abstracts = data[\"Abstract\"]\n",
    "\n",
    "# Define NLP model\n",
    "nlp = spacy.load('en_core_web_trf', disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "\n",
    "# List to store processed Doc objects\n",
    "processed_abstracts = []\n",
    "\n",
    "for abstract in abstracts:\n",
    "    doc = nlp(abstract)  # Process the abstract\n",
    "    processed_abstracts.append(doc)  # Save the processed doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all locations (GPE and LOC) from the processed documents \n",
    "\n",
    "# Dictionary to store location entities for each abstract\n",
    "locations = {}  \n",
    "\n",
    "# Extract location entities from saved docs\n",
    "for idx, doc in enumerate(processed_abstracts):\n",
    "    location_set = set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'GPE' or ent.label_ == 'LOC':\n",
    "            words = ent.text.split()\n",
    "            filtered_location = \" \".join(word for word in words if word.lower() not in STOP_WORDS)\n",
    "            if filtered_location:\n",
    "                location_set.add(filtered_location)\n",
    "\n",
    "    locations[idx] = list(location_set)\n",
    "\n",
    "# Combine unique locations from all abstracts into a single set\n",
    "all_locations = set()\n",
    "for loc_list in locations.values():\n",
    "    all_locations.update(loc_list)\n",
    "\n",
    "# Print all unique locations\n",
    "print(\"Unique Locations:\")\n",
    "for location in all_locations:\n",
    "    print(location)\n",
    "\n",
    "print(\"Number of unique locations:\", len(all_locations))\n",
    "\n",
    "# Save the locations for each document\n",
    "data_loc = pd.DataFrame.from_dict(locations, orient='index')\n",
    "data_loc.index.name = 'Index'\n",
    "data_loc.to_csv(r'..\\NER\\data_loc.csv')\n",
    "\n",
    "# Save all_locations as a CSV file using pandas\n",
    "all_locations_df = pd.DataFrame({'Location': list(all_locations)})\n",
    "all_locations_df.to_csv(r'..\\NER\\all_locations.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get coordinates for each location using the GeoNames API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all locations.csv and get a list from it\n",
    "all_locations_df = pd.read_csv(r'..\\NER\\all_locations.csv')\n",
    "all_loc = all_locations_df['Location']\n",
    "\n",
    "# Username obtained from GEONAMES\n",
    "import helper\n",
    "config = helper.read_config()\n",
    "\n",
    "username = config['GEONAMESSettings']['username']\n",
    "\n",
    "# Dictionary to store the location information\n",
    "location_data = {}\n",
    "\n",
    "# List to store error locations\n",
    "error_locations = []\n",
    "\n",
    "# Set delay to conform with API limitations\n",
    "delay = 5\n",
    "\n",
    "# Iterate over the location names\n",
    "for location_name in all_loc:\n",
    "    try:\n",
    "        # Construct the search API URL\n",
    "        url = f\"http://api.geonames.org/searchJSON?name={location_name}&fuzzy=0.8&maxRows=50&username={username}\"\n",
    "\n",
    "        # Make the search request\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract the relevant information for the location\n",
    "        if \"geonames\" in data and len(data[\"geonames\"]) > 0:\n",
    "            location = data[\"geonames\"][0]\n",
    "            feature_class = location.get(\"fcl\", \"Unknown\")\n",
    "            latitude = location.get(\"lat\", \"Unknown\")\n",
    "            longitude = location.get(\"lng\", \"Unknown\")\n",
    "\n",
    "            # Store the information in the location_data dictionary\n",
    "            location_data[location_name] = {\n",
    "                \"location\": \"exact\",\n",
    "                \"feature_class\": feature_class,\n",
    "                \"latitude\": latitude,\n",
    "                \"longitude\": longitude,\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            # Extract individual words from the location\n",
    "            words = location_name.split()\n",
    "\n",
    "            for word in words:\n",
    "                # Construct the search API URL for individual words\n",
    "                url = f\"http://api.geonames.org/searchJSON?name={word}&fuzzy=0.8&maxRows=50&username={username}\"\n",
    "\n",
    "                # Make the search request\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Parse the JSON response\n",
    "                data = response.json()\n",
    "\n",
    "                # Check if a match is found\n",
    "                if \"geonames\" in data and len(data[\"geonames\"]) > 0:\n",
    "                    matched_location = data[\"geonames\"][0]\n",
    "                    matched_feature_class = matched_location.get(\"fcl\", \"Unknown\")\n",
    "                    matched_latitude = matched_location.get(\"lat\", \"Unknown\")\n",
    "                    matched_longitude = matched_location.get(\"lng\", \"Unknown\")\n",
    "\n",
    "                    # Store the information in the location_data dictionary\n",
    "                    location_data[location_name] = {\n",
    "                        \"location\":matched_location.get(\"name\", \"Unknown\"),\n",
    "                        \"feature_class\": matched_feature_class,\n",
    "                        \"latitude\": matched_latitude,\n",
    "                        \"longitude\": matched_longitude,\n",
    "                    }\n",
    "                    break  # Stop searching for individual words if a match is found\n",
    "\n",
    "            if location_name not in location_data:\n",
    "                # Location not found\n",
    "                error_locations.append(location_name)\n",
    "                print(f\"Location '{location_name}' not found in GeoNames database\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_locations.append(location_name)  # Append location name to error list\n",
    "        print(f\"An error occurred for location '{location_name}': {str(e)}\")\n",
    "        continue  # Continue to the next location\n",
    "    \n",
    "    # Delay between consecutive requests\n",
    "    time.sleep(delay)\n",
    "\n",
    "# Save location data (feature, lat, lon) for each unique location\n",
    "location_df = pd.DataFrame.from_dict(location_data, orient='index')\n",
    "location_df.index.name = 'Location'\n",
    "location_df.to_csv(r'..\\NER\\fuzzy_location_data.csv')\n",
    "\n",
    "df_error = pd.DataFrame({'Error Locations': error_locations})\n",
    "df_error.to_csv(r'..\\NER\\fuzzy_error_locations.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute each location to an ocean basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance to the nearest ocean/sea\n",
    "\n",
    "# Import math funtions\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "# Load the data\n",
    "location_df = pd.read_csv(r'..\\NER\\fuzzy_location_data.csv')\n",
    "range_area = pd.read_csv(r'..\\NER\\range_area.csv')\n",
    "\n",
    "# Filter instances with latitude outside the valid range\n",
    "invalid_latitudes = range_area[(range_area['Latitude'] < -90) | (range_area['Latitude'] > 90)]\n",
    "\n",
    "# Filter instances with longitude outside the valid range\n",
    "invalid_longitudes = range_area[(range_area['Longitude'] < -180) | (range_area['Longitude'] > 180)]\n",
    "\n",
    "# Get the indices of instances with invalid latitude or longitude\n",
    "invalid_indices = invalid_latitudes.index.union(invalid_longitudes.index)\n",
    "\n",
    "# Remove instances with invalid latitude or longitude from range_area\n",
    "range_area_cleaned = range_area.drop(invalid_indices)\n",
    "\n",
    "# Print the cleaned range_area dataframe\n",
    "print(range_area_cleaned)\n",
    "\n",
    "# The haversine formula calculates the distance between two point in a sphere\n",
    "\"\"\"As the Earth is nearly spherical, the haversine formula provides a good\n",
    "    approximation of the distance between two points of the Earth surface, with\n",
    "    a less than 1% error on average.\"\"\"\n",
    "\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Convert coordinates from degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    distance = 6371 * c  # Radius of the Earth in kilometers\n",
    "    return distance\n",
    "\n",
    "def find_closest_ocean_basin(df_locations, df_ocean_basins):\n",
    "    closest_ocean_basins = []\n",
    "\n",
    "    for _, loc_row in df_locations.iterrows():\n",
    "        min_distance = float('inf')\n",
    "        closest_ocean_basin = None\n",
    "\n",
    "        for _, ocean_row in df_ocean_basins.iterrows():\n",
    "            try:\n",
    "                distance = haversine(loc_row['latitude'], loc_row['longitude'], ocean_row['Latitude'], ocean_row['Longitude'])\n",
    "\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    closest_ocean_basin = ocean_row['range_area']\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred: {e}\")\n",
    "\n",
    "        closest_ocean_basins.append((closest_ocean_basin, min_distance))\n",
    "\n",
    "    return closest_ocean_basins\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Assuming you have two dataframes: location_df and ocean_basins\n",
    "\n",
    "# location_df contains location names, latitude, and longitude\n",
    "# location_df = pd.DataFrame({\n",
    "#     'Location': ['Location A', 'Location B', 'Location C'],\n",
    "#     'latitude': [40.7128, 34.0522, 51.5074],\n",
    "#     'longitude': [-74.0060, -118.2437, -0.1278]\n",
    "# })\n",
    "\n",
    "# ocean_basins contains ocean basins or seas with their labels, latitude, and longitude\n",
    "# ocean_basins = pd.DataFrame({\n",
    "#     'range_area': ['Atlantic Ocean', 'Pacific Ocean', 'Mediterranean Sea'],\n",
    "#     'Latitude': [37.0902, -8.7832, 35.8984],\n",
    "#     'Longitude': [-95.7129, -124.0198, 14.5120]\n",
    "# })\n",
    "\n",
    "# Call the function to find the closest ocean basin or sea for each location\n",
    "closest_ocean_basins = find_closest_ocean_basin(location_df, range_area)\n",
    "\n",
    "# Unpack the tuples into separate lists for ocean basin and distance\n",
    "closest_ocean_labels, distances = zip(*closest_ocean_basins)\n",
    "\n",
    "# Add the closest ocean basin and distance information to the df_locations dataframe\n",
    "location_df['Closest Ocean Basin'] = closest_ocean_labels\n",
    "location_df['Distance to Nearest Ocean'] = distances\n",
    "location_df.to_csv(r'..\\NER\\fuzzy_location_data.csv')\n",
    "\n",
    "# Print the updated dataframe\n",
    "print(location_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check how many locations are less than 50, 100, 500, 1000 km from the coast\n",
    "\n",
    "# Initialize counters\n",
    "count_less_than_50 = 0\n",
    "count_50_to_100 = 0\n",
    "count_100_to_500 = 0\n",
    "count_500_to_1000 = 0\n",
    "\n",
    "# Iterate over the distances list\n",
    "for distance in location_df['Distance to Nearest Ocean']:\n",
    "    if distance < 50:\n",
    "        count_less_than_50 += 1\n",
    "    if 50 < distance < 100:\n",
    "        count_50_to_100 += 1\n",
    "    if 100 < distance < 500:\n",
    "        count_100_to_500 += 1\n",
    "    if 500 < distance < 1000:\n",
    "        count_500_to_1000 += 1\n",
    "# Print the counts\n",
    "print(\"Number of instances less than 50 km: \", count_less_than_50)\n",
    "print(\"Number of instances less than 100 km: \", count_50_to_100)\n",
    "print(\"Number of instances less than 500 km: \", count_100_to_500)\n",
    "print(\"Number of instances less than 1000 km: \", count_500_to_1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify each paper by ocean basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = pd.read_csv(r'..\\NER\\data_loc.csv')\n",
    "location_df = pd.read_csv(r'..\\NER\\fuzzy_location_data.csv')\n",
    "print(data_loc.head())\n",
    "print(location_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping locations to labels\n",
    "location_dict = location_df.set_index('Location')['Closest Ocean Basin'].to_dict()\n",
    "\n",
    "# Create a new dataframe with only labels for each location in data_loc\n",
    "labels_df = data_loc.copy()  # Make a copy of data_loc\n",
    "\n",
    "# Iterate over each column in labels_df\n",
    "for col in labels_df.columns[1:]:  # Exclude the first column (Index)\n",
    "    labels_df[col] = labels_df[col].map(location_dict)\n",
    "\n",
    "# Print the new dataframe\n",
    "print(labels_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recurrent value per row in labels_df, excluding the index\n",
    "classes = []\n",
    "\n",
    "for _, row in labels_df.iterrows():\n",
    "    try:\n",
    "        classes.append(row.drop(labels='Index').value_counts().idxmax())\n",
    "    except ValueError:\n",
    "        classes.append(None)\n",
    "\n",
    "# Create a new dataframe with only the most recurrent value per row\n",
    "classes = pd.DataFrame(classes, columns=['Most Recurrent Value'])\n",
    "\n",
    "# Print the new dataframe\n",
    "print(classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## Fix some issues with the basins dataset\n",
    "range_area = pd.read_csv(r'..\\NER\\range_area.csv')\n",
    "\n",
    "# Convert the 'range_area' column to float\n",
    "range_area['range_area'] = range_area['range_area'].astype(float)\n",
    "\n",
    "# Filter instances with latitude outside the valid range\n",
    "invalid_latitudes = range_area[(range_area['Latitude'] < -90) | (range_area['Latitude'] > 90)]\n",
    "\n",
    "# Filter instances with longitude outside the valid range\n",
    "invalid_longitudes = range_area[(range_area['Longitude'] < -180) | (range_area['Longitude'] > 180)]\n",
    "\n",
    "# Get the indices of instances with invalid latitude or longitude\n",
    "invalid_indices = invalid_latitudes.index.union(invalid_longitudes.index)\n",
    "\n",
    "# Remove instances with invalid latitude or longitude from range_area\n",
    "range_area = range_area.drop(invalid_indices)\n",
    "\n",
    "## Remove differentiation between coastal and oceanic basins (i.e. coastal north atlantic and north atlantic)\n",
    "merge_classes = [[2,3,1], [4,5], [6,7],[8,9], [10,11], [12,13], [14,15], [16,17], [18,19]]\n",
    "\n",
    "# Create the mapping dictionary for merging\n",
    "merge_mapping = {}\n",
    "for merged_class in merge_classes:\n",
    "    merged_label = merged_class[0]\n",
    "    for class_label in merged_class:\n",
    "        merge_mapping[class_label] = merged_label\n",
    "\n",
    "# Merge the classes based on the mapping\n",
    "classes['Merged Class'] = classes['Most Recurrent Value'].replace(merge_mapping)\n",
    "\n",
    "# Calculate the basin counts\n",
    "basin_counts = classes['Merged Class'].value_counts()\n",
    "\n",
    "# Merge the classes based on the mapping in range_area DataFrame\n",
    "range_area['Merged Class'] = range_area['range_area'].replace(merge_mapping)\n",
    "\n",
    "# Map the basin counts to the 'range_area' column\n",
    "range_area['basin_count'] = range_area['Merged Class'].map(basin_counts)\n",
    "\n",
    "# Include ocean label besides code\n",
    "range_basin_list = pd.read_csv(r'..\\NER\\range_basin_list.csv', names=['range_area', 'basin'])\n",
    "range_basin_list['range_area'] = range_basin_list['range_area'].astype(float).round(1)\n",
    "range_area = range_area.merge(range_basin_list, on='range_area', how='left')\n",
    "range_area.drop(['range_area','Merged Class'], axis=1, inplace=True)\n",
    "\n",
    "# Create a dictionary to map range_area to basin\n",
    "label_mapping = dict(zip(range_basin_list['range_area'], range_basin_list['basin']))\n",
    "\n",
    "# Map the values in 'merged_classes' to labels\n",
    "classes['Basin'] = classes['Merged Class'].map(label_mapping)\n",
    "\n",
    "range_area.to_csv(r'..\\NER\\maps\\range_area_counts.csv', index=False)\n",
    "classes.to_csv(r'..\\NER\\merged_fuzzy_classes_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classes['Basin'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check some info on location matches\n",
    "sum_all = 0\n",
    "\n",
    "for location in location_df['location']:\n",
    "    if location == 'exact':\n",
    "        sum_all += 1\n",
    "print('The number of exact location matches is:' + str(sum_all))\n",
    "\n",
    "sum_all = 0\n",
    "\n",
    "for location in location_df['location']:\n",
    "    if location != 'exact':\n",
    "        sum_all += 1\n",
    "print('The number of separated words location matches is:' + str(sum_all))\n",
    "\n",
    "sum_all = 0\n",
    "classes.fillna(\"NaN\",inplace=True)\n",
    "for location in classes['Most Recurrent Value']:\n",
    "    if location != 'NaN':\n",
    "        sum_all += 1\n",
    "print('The number of papers with locations is:' + str(sum_all))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
